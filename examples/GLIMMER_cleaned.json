{"paper_id": "GLIMMER", "title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization", "abstract": "Pre-trained language models are increasingly being used in multi-document summarization tasks. However, these models need largescale corpora for pre-training and are domaindependent. Other non-neural unsupervised summarization approaches mostly rely on key sentence extraction, which can lead to information loss. To address these challenges, we propose a lightweight yet effective unsupervised approach called GLIMMER: a Graph and LexIcal features based unsupervised Multi-docuMEnt summaRization approach. It first constructs a sentence graph from the source documents, then automatically identifies semantic clusters by mining low-level features from raw texts, thereby improving intra-cluster correlation and the fluency of generated sentences. Finally, it summarizes clusters into natural sentences. Experiments conducted on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach outperforms existing unsupervised approaches. Furthermore, it surpasses state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally, human evaluations indicate that summaries generated by GLIMMER achieve high readability and informativeness scores. Our code is available at https://github.com/ Oswald1997/GLIMMER.", "pdf_parse": {"paper_id": "GLIMMER", "abstract": [{"text": "Pre-trained language models are increasingly being used in multi-document summarization tasks. However, these models need largescale corpora for pre-training and are domaindependent. Other non-neural unsupervised summarization approaches mostly rely on key sentence extraction, which can lead to information loss. To address these challenges, we propose a lightweight yet effective unsupervised approach called GLIMMER: a Graph and LexIcal features based unsupervised Multi-docuMEnt summaRization approach. It first constructs a sentence graph from the source documents, then automatically identifies semantic clusters by mining low-level features from raw texts, thereby improving intra-cluster correlation and the fluency of generated sentences. Finally, it summarizes clusters into natural sentences. Experiments conducted on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach outperforms existing unsupervised approaches. Furthermore, it surpasses state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally, human evaluations indicate that summaries generated by GLIMMER achieve high readability and informativeness scores. Our code is available at https://github.com/ Oswald1997/GLIMMER.", "section": "Abstract", "sec_num": null}], "body_text": [{"text": "Multi-document summarization (MDS) aims to produce a summary from a document set containing a series of related topics. The generated summary needs to cover all important information in the document set, while remaining fluent and concise. Compared with single-document summarization, it's more challenging because an increasing number of input documents will make the source information more redundant and scattered. It also has practical significance, for example, key information of multiple news articles can be generated efficiently. Multi-document summarization approaches can also be applied in other scenarios, such as extracting opinions from social media.", "section": "Introduction", "sec_num": "1"}, {"text": "As the application of MDS becomes more and more widespread, various approaches have been proposed. Non-neural approaches are primarily based on extracting key sentences (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Rossiello et al., 2017) . These approaches assess sentence importance based on their relevance to each other or proximity to keywords, selecting sentences with high importance scores to form the summary. The main drawback of such approaches is that they retain only a subset of key sentences, which can lead to information loss and may not capture finegrained details.", "section": "Introduction", "sec_num": "1"}, {"text": "Neural approaches can generate more abstractive text and are recently widely used in multidocument summarization. Given the structural characteristics of the multi-document input, most approaches utilize attention mechanism to build hierarchical models (Fabbri et al., 2019; Mao et al., 2020; Jin et al., 2020) , enabling the extraction of different-grained features and the selection of important information. Other methods employ graphs to model relationships and can leverage interaction features to enhance representation (Yasunaga et al., 2017; Yin et al., 2019; Li et al., 2020) . Despite their capability in extracting abstract features, neural models are often resource-intensive and require large parallel training datasets. Moreover, recent studies (Pagnoni et al., 2021) have shown that they also suffer from factuality problems.", "section": "Introduction", "sec_num": "1"}, {"text": "Pre-trained language models use large-scale cor- pora to optimize objective functions, allowing them to acquire more general feature representations. This capability enables them to require only a small amount of data to fine-tune downstream tasks and achieve impressive results. Among various language models, BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) excel particularly in generation tasks. Some models leverage them as backbone and generate high-quality summaries (Pasunuru et al., 2021; Liu et al., 2022) . However, these pre-trained models are designed for general generation tasks without specific optimizations for MDS. To address this limitation, some researchers have modified objective functions during pre-training to better align models with this task (Zhang et al., 2020a; Xiao et al., 2022) . Despite being the latest models, they have drawbacks such as the requirement for largescale corpora and relatively low summarization efficiency. Moreover, the knowledge acquired during pre-training may introduce unwanted external factors that can affect the quality of summaries.", "section": "Introduction", "sec_num": "1"}, {"text": "Given the challenges observed in various existing approaches, we aim to develop a general, fast, and easy-to-use MDS model that can generate highquality summaries without annotated data or extensive computational resources. Existing supervised models rely heavily on advanced features such as word vectors and outputs from hidden layers of neural networks, overlooking important low-level features present in raw texts. Therefore, our motivation is to leverage these low-level features, such as lexical features, from raw texts to rapidly mine information and achieve unsupervised summariza-tion. We introduce GLIMMER, illustrated in Figure 1 , which constructs a sentence graph to represent interaction relations. Unlike the graph-based methods mentioned earlier, our approach does not rely on neural layers to handle graph features. Instead, it identifies semantic clusters from the graph in an unsupervised manner. Specifically, we mine lexical features from raw texts to facilitate graph cut. Subsequently, each semantic cluster is transformed into an informative and coherent summary to generate the final output. Our primary contributions are outlined as follows:", "section": "Introduction", "sec_num": "1"}, {"text": "\u2022 GLIMMER is a fully unsupervised multidocument summarization approach that can run on a CPU. As an out-of-the-box solution, it does not require labeled samples, large-scale corpus or additional settings for hyperparameters, and demonstrates superiority in realworld scenarios.", "section": "Introduction", "sec_num": "1"}, {"text": "\u2022 We utilize low-level lexical features from raw texts to automatically determine the number of semantic clusters. To our knowledge, we are the first to employ these characteristics in multi-document summarization.", "section": "Introduction", "sec_num": "1"}, {"text": "\u2022 Experiments conducted on Multi-News, Multi-XScience, and DUC-2004 demonstrate that our approach outperforms other nonneural approaches based on automatic evaluation metrics, as well as state-of-the-art pretrained multi-document summarization models in zero-shot scenarios. Human evaluation indicates that summaries generated by GLIM-MER are more readable and factually accurate than those generated by previous non-neural approaches.", "section": "Introduction", "sec_num": "1"}, {"text": "Graph-based Text Summarization Graphs are effective in representing relations and distances between nodes, making them well-suited for modeling text and extracting specific features. Christensen et al. (2013) constructed an approximate discourse graph (ADG) based on discourse relations across sentences. They utilized ADG for selecting summary sentences and achieved more coherent summaries. Subsequent work has expanded upon ADG (Yasunaga et al., 2017; Liu and Lapata, 2019) due to its ability to capture correlation features. Despite the capability of these summarization models to capture graph features, they still require training network layers to bridge the gap between features and outputs. Moreover, a large amount of supervised data is needed to train these parameters. Topic Modelling When identifying semantic clusters, topic models are intuitively suitable. They mine latent topics from document sets to achieve semantic clustering. Classic topic models such as Latent Dirichlet Allocation (Blei et al., 2003) and subsequent neural topic models (Srivastava and Sutton, 2017; Bianchi et al., 2021) require the definition of the number of topics. Despite various metrics available to evaluate the quality of mined topics (Terragni et al., 2021) , determining the optimal number of topics remains challenging. BERTopic (Grootendorst, 2022) addresses this by using a hierarchical clustering algorithm to identify clusters of varying densities. However, when applied to MDS, its clustering performance is inadequate. This limitation may stem from the fact that while topic models excel at distinguishing texts with different topics, they struggle with semantic clustering where texts share similar topics. In our work, we address this challenge by constructing graphs and leveraging lexical features from raw texts, which proves effective in this scenario.", "section": "Related Work", "sec_num": "2"}, {"text": "Text Compression Some researchers focus on unsupervised sentence compression techniques. F\u00e9vry and Phang (2018) add noise to original sentences and use denoising auto-encoders to recover them. Ghalandari et al. (2022) employ reinforcement learning to generate fluent compressed sentences of appropriate length. While these approaches are valuable, they are limited to compress-ing a single sentence and may fail when dealing with clusters of related sentences. To address this challenge, word graph-based methods offer a potential solution. Nodes and edges are constructed based on specific rules, and constraints are applied to identify the most suitable compression path (Filippova, 2010; Boudin and Morin, 2013; Mehdad et al., 2013; Shang et al., 2018) . In our work, we integrate this concept as one module of GLIMMER and modify it to suit multi-document summarization tasks.", "section": "Related Work", "sec_num": "2"}, {"text": "Figure 1 illustrates the framework of GLIMMER.", "section": "GLIMMER Framework", "sec_num": "3"}, {"text": "Each input comprises multiple source documents and has been segmented into sentences. Subsequently, a sentence graph is constructed where each node represents a sentence, and each edge represents the relation between two nodes. Graph cut is then applied to the affinity matrix to identify subgraphs based on lexical features, with each subgraph representing a semantic cluster. Next, we construct a directed word graph for each cluster. By selecting the most suitable path within each graph, we generate informative and fluent summary sentences. The three main modules of GLIMMER are detailed below.", "section": "GLIMMER Framework", "sec_num": "3"}, {"text": "Given that ADG can effectively describe discourse relations and reflect the degree of correlation, it aligns well with our task. The original implementation of ADG relies on specific indicators such as co-reference and discourse cues, it also needs statistics from large corpus. To streamline the process and enhance the effectiveness of subsequent graph cut operations, we modify ADG and construct sentence graph based on the following four indicators: Deverbal Noun Reference A verb is associated with describing an event, and in subsequent sentence, the nominalization form of this verb is usually used to refer to the event. This type of expression can serves as an indicator to connect related sentences. Specifically, we extract verbs from a sentence, 1 filtering out non-notional verbs such as was or had. We then use WordNet to obtain all relevant nominalization forms of verbs. 2 Next, we identify the most similar words for these nouns using word vectors, resulting in a list of nouns to be matched. When processing subsequent sentences, we compare their nouns to the list generated earlier.", "section": "Sentence Graph Construction", "sec_num": "3.1"}, {"text": "A match indicates that these two sentences contain a deverbal noun reference relation.", "section": "Sentence Graph Construction", "sec_num": "3.1"}, {"text": "Conjunctions A conjunction is used to join two sentences, indicating a relationship between them. This relationship can be either coordinative or adversative, suggesting that the sentences are related to the same topic. In our implementation, we identify 39 conjunctions. If a subsequent sentence begins with one of these conjunctions, then the two sentences are considered to match successfully. Entity Consistency If two sentences have the same entity, it means they likely refer to the same event, indicating semantic correlation. To implement this, we extract named entities from two sentences. If they contain the same entity with the same entity type, we consider it a successful match.", "section": "Sentence Graph Construction", "sec_num": "3.1"}, {"text": "Semantic Similarity This is a straightforward indicator. We calculate sentence vectors and use cosine value to measure the similarity between two sentences. If cosine similarity exceeds a certain threshold, we consider the match successful.", "section": "Sentence Graph Construction", "sec_num": "3.1"}, {"text": "Based on the aforementioned four indicators, we construct graph (V, E), where each node v i \u2208 V represents a single sentence, and each edge e i,j \u2208 E indicates connection status between node v i and node v j . We set e i,j = 1 if at least one of the four indicators is satisfied, or e i,j = 0 if none conditions is met. Note that we only use deverbal noun reference and conjunctions indicators for adjacent sentences, as it is not meaningful when sentences are far apart in text. We don't set edge weights like Christensen et al. (2013) , because although weighted ADG can aid in path selection and contribute to get coherent summary sentences, it shows limited improvement in subsequent graph cut operations.", "section": "Sentence Graph Construction", "sec_num": "3.1"}, {"text": "After finishing the construction of the sentence graph, we obtain an n * n adjacency matrix, where n is equal to the number of input sentences. This matrix can be considered as an affinity matrix and is subjected to graph cut to identify normalized graph cuts, with each resulting subgraph representing a semantic cluster. As discussed in \u00a72, determining the appropriate number of semantic clusters is challenging. Therefore, in this section, we propose two methods to address this issue, called TTR-based method and distance-based method, respectively. Then graph cut can be conducted.", "section": "Semantic Cluster Identification", "sec_num": "3.2"}, {"text": "Lexical features can reflect semantic information to some extent. Among different features, type-token ratio (TTR) is one of the most commonly used feature and it can measure lexical richness. The formula is as follows where T represents the number of unique words and N represents the total number of words.", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "T T R = T N (1)", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "Intuitively, TTR is positively correlated with semantic richness. This is because richer semantic information often leads to a richer vocabulary, whereas semantically poor expressions tend to contain more repetitive words. Similarly, a larger number of semantic clusters tends to imply richer semantic information. As a consequence, TTR of the input text is also positively correlated with the number of semantic clusters or subgraphs. A simple formula to estimate number of clusters can be expressed as follows:", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "EQUATION", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "The formula calculates the number of semantic clusters based on the product of the whole input's TTR value and the number of sentences in the input. In an extreme case where TTR equals one, each sentence in the input exhibits high semantic richness and is distinct from others, resulting in each sentence forming a semantic cluster independently. However, this formula assumes equal contribution from all input sentences when calculating the number of clusters, thereby overlooking the varying influences of different sentences on semantic richness. Specifically, sentences with higher TTR values contain richer information, leading to a greater number of clusters, and vice versa. To address this, we differentiate between high-TTR and low-TTR sentences. We refer to McKee et al. (2000) , who proposed a method to illustrate the relation between TTR and sample size, the formula is as follows:", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "EQUATION", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "In this formula, D is used as a parameter of calculating lexical richness. It is estimated from original input, please refer to Appendix D for detailed information on the estimation process. Once D is determined, we apply Formula (3) to each sentence of the input to obtain the estimated TTR value for each sentence. Additionally, we calculate the true TTR value for each sentence using Formula (1).", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "T T R true T T R esti < 1 -\u03c3 (4) T T R true T T R esti \u2a7e 1 + \u03c3 (5)", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "If a sentence from the input satisfies Formula (4), it is considered a low-TTR sentence, otherwise, it is classified as a high-TTR sentence. Furthermore, we extend Formula (2) to Formula ( 6), where n low and n high represent the numbers of low-TTR and high-TTR sentences, respectively. \u03b2 is an influence factor that controls the degree to which different sentences affect the semantic richness of the overall input.", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "n cluster = \u230a[n sent -\u03b2(n low -n high )] * T T R input \u230b (6)", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "Distance-based Method This method is much simpler and more intuitive than TTR-based method, it utilizes the adjacency matrix calculated from \u00a73.1. The core idea is similar to silhouette coefficient, aiming to minimize the average distance between nodes within the same cluster and maximize the average distance between nodes in different clusters.", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "To implement this method, we vary the number of clusters and apply spectral clustering algorithm to obtain different graph cut results. We then use Floyd-Warshall algorithm to calculate distances between nodes and determine the optimal number of clusters that satisfy the aforementioned requirements.", "section": "Clustering Number Determination TTR-based Method", "sec_num": "3.2.1"}, {"text": "After obtaining the optimal number of semantic clusters using either the TTR-based or distancebased method, we perform graph cut on the affinity matrix to find normalized graph cuts. Since our affinity matrix and adjacency matrix are identical, there is no need to construct a similarity graph. We calculate Laplacian matrix L using Formula (7), where W is adjacency matrix and D is degree matrix.", "section": "Graph Cut", "sec_num": "3.2.2"}, {"text": "EQUATION", "section": "Graph Cut", "sec_num": "3.2.2"}, {"text": "Next, we calculate first k eigenvectors u 1 , u 2 , ..., u k of L, where k is the number of clusters that we have determined. We then construct eigenmatrix U \u2208 R n * k , which consists of eigenvectors u 1 , u 2 , ..., u k as columns. Finally, we apply k-means algorithm to partition the n sentences into k clusters, resulting in the formation of semantic clusters.", "section": "Graph Cut", "sec_num": "3.2.2"}, {"text": "We perform multi-sentence summarization on each semantic cluster to generate a concise summary which is also informative and fluent.", "section": "Semantic Cluster Summarization", "sec_num": "3.3"}, {"text": "The first step is to construct a word graph for each cluster. Figure 1 contains an example of word graph. Each graph has a start node and an end node, and each sentence in the cluster is connected between these two special nodes sequentially and separately. The edges in the graph follow the direction of natural language, with each node representing a single word. To illustrate interactions among sentences within the same cluster, words with the same lowercase form and part of speech are mapped to identical nodes. As a result, the number of paths between the start and end nodes becomes significantly larger than the number of sentences in the cluster. It's important to note that no two words within a single sentence will be mapped to the same node. In cases where there are multiple choices for mapping subsequent words to the graph, we examine the context of the nodes to select the node with the most context coincidence or the highest mapping frequency.", "section": "Semantic Cluster Summarization", "sec_num": "3.3"}, {"text": "For edge weights, we refer to Filippova (2010) :", "section": "Semantic Cluster Summarization", "sec_num": "3.3"}, {"text": "EQUATION", "section": "Semantic Cluster Summarization", "sec_num": "3.3"}, {"text": "f (i) represents frequency or mapped times of node i. D s (i, j) -1 denotes the inverse value of positional distance between node i and j in sentence s, which is meaningful only when node i precedes node j. The left term indicates that when selecting the shortest path, preference is given to edges where nodes have low frequencies but strong connections. The right term suggests that important nodes are more likely to be selected. In this way, the selected path contains both salient words and specific expressions, making the summary of a cluster informative.", "section": "Semantic Cluster Summarization", "sec_num": "3.3"}, {"text": "Once edge weights are determined, we select the shortest path from start node to end node, which has the smallest sum of edge weights. Additional, to further improve fluency, we divide the sum of edge weights by the sum of n-gram probabilities for each path. In this way, we reselect path with the lowest score. After summarizing of all semantic clusters, we obtain a final summary of the input documents set.", "section": "Semantic Cluster Summarization", "sec_num": "3.3"}, {"text": "We use Multi-News (Fabbri et al., 2019) , Multi-XScience (Lu et al., 2020) and DUC-2004 (Over and Yen, 2004) as datasets, all of which are commonly-used MDS datasets. We have verified that these datasets do not contain sensitive data in terms of privacy and security. As an unsupervised approach, we only use test set of the above datasets, with document set sizes of 5622, 5093 and 50 respectively. More descriptions of datasets can be found in Appendix A.1.", "section": "Datasets", "sec_num": "4.1"}, {"text": "Our baselines are First-n, LexRank (Erkan and Radev, 2004) , Centroid (Rossiello et al., 2017) , Summpip (Zhao et al., 2020) , PEGASUS (Zhang et al., 2020a) , PRIMERA (Xiao et al., 2022) . Please refer to Appendix A.2 for details.", "section": "Baselines", "sec_num": "4.2"}, {"text": "Inspired by determining clustering number based on eigengap, we replace our TTRbased and distance-based methods with eigengapbased method and obtain a new baseline called GLIMMER-Eigengap.", "section": "Baselines", "sec_num": "4.2"}, {"text": "Since Multi-News dataset provided by Fabbri et al. (2019) has been tokenized, we tokenize Multi-XScience and DUC-2004 as well to standardize the data format and facilitate performance comparison across different approaches.", "section": "Experiment Design", "sec_num": "4.3"}, {"text": "Following Xiao et al. (2022) , we control output lengths of baselines to ensure fair comparison. As the performance of each baseline model varies significantly across different datasets depending on hyperparameters, we adjusted these hyperparameters separately for each dataset to ensure that baselines performs well across different datasets. For more details, please refer to Appendix A.", "section": "Experiment Design", "sec_num": "4.3"}, {"text": "For GLIMMER, we set thresholds for identifying similar words and matching similar sentences in \u00a73.1 to 0.65 and 0.98 respectively. Additionally, We set \u03c3 = 0.05 and \u03b2 = 4 for TTR-based method.", "section": "Experiment Design", "sec_num": "4.3"}, {"text": "We use ROUGE scores to automatically evaluate summarization performance. Specifically, we use f 1 of ROUGE-1, ROUGE-2 and ROUGE-L,3 which take into account completeness, readability and order of summaries.", "section": "Automatic Evaluation", "sec_num": "4.4"}, {"text": "Table 1 presents results of GLIMMER and baselines. Our approach achieves the best results on two-thirds of metrics, indicating improvements in coverage and word sequence quality in multidocument summarization. GLIMMER performs particularly well on two datasets, including Multi-XScience, which is more challenging due to its higher abstractiveness. This demonstrates that GLIMMER is not only suitable for news articles but can also be applied to other fields such as technical articles. Furthermore, results of GLIMMER surpass non-neural unsupervised approaches and largescale pre-trained models. However, GLIMMER-Eigengap performs poorly on Multi-News, suggesting that this traditional method for determining clustering numbers does not generalize well across different tasks. However, we observed that pre-trained models like PEGASUS and PRIMERA do not outperform traditional non-neural network approaches, despite being considered SOTA summarization models. This could be attributed to their pre-training on large corpora, which may have distributions different from that of the test sets. Table 2 indicates that only when PRIMERA is fine-tuned using supervised samples does its excellent feature extraction ability become apparent. However, even under few-shot scenarios, GLIMMER's ROUGE scores remain competitive with those of fine-tuned models.", "section": "Automatic Evaluation", "sec_num": "4.4"}, {"text": "In addition, GLIMMER also achieves highest scores on BERTScore (Zhang et al., 2020b ),4 even outperforming fully fine-tuned PRIMERA (Table 3). We also use a neural evaluation framework called UniEval (Zhong et al., 2022) to automatically assess the readability of summaries. While not as accurate as human evaluation, we consider this an additional experiment, and the results can be found in Appendix C.", "section": "Automatic Evaluation", "sec_num": "4.4"}, {"text": "Table 1 : Rouge scores of different models on Multi-News, Multi-XScience and DUC-2004. GLIMMER-TTR and GLIMMER-Distance indicates our approach utilizes TTR-based and distance-based method described in \u00a73.2 respectively. Best results under each category have been bolded (statistical significance with p-value < 0.05). Results of PEGASUS on all datasets, PRIMERA on Multi-XScience and DUC-2004 are from Xiao et al. (2022) . ", "section": "Automatic Evaluation", "sec_num": "4.4"}, {"text": "Multi-XScience DUC-2004 model R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-", "section": "Multi-News", "sec_num": null}, {"text": "Automatic evaluation alone does not provide a comprehensive assessment of summarization quality, as readability and informativeness are also important factors to consider. We use fluency, coherence, and referential clarity as indicators to evaluate readability. Together with informativeness, these four indicators are used as the basis for human evaluation on 50 random samples. For more details on the human evaluation process, please refer to Appendix B.1. Table 4 presents the average evaluation results from three annotators for different models. Summpip shows the worst overall performance, while our approach demonstrates significant improvement over previous non-neural methods. PRIMERA, being a large language model, achieves the highest readability scores, which is expected given its extensive language knowledge learned during pre-training and its abstractive nature. GLIMMER-TTR closely follows PRIMERA, also obtaining relatively high readability scores. Additionally, its informativeness score significantly exceeds that of PRIMERA, suggesting that the summaries generated by our model not only contain more information but also match the state-of-theart neural model in terms of readability. For more details regarding agreement analysis and score distributions, please refer to the Appendix B.", "section": "Human Evaluation", "sec_num": "4.5"}, {"text": "The basic structure of GLIMMER consists of three main components: sentence graph construction, semantic clustering, and cluster summarization. In the second module, one of our main contributions is devising methods to automatically determine the number of clusters. In our ablation study, similar to most clustering algorithms, we set the number of clusters to a fixed value, which is a commonly used approach. Then, in the third module, we select the shortest path based solely on path weights, disregarding fluency factor. Results of ablation study on Multi-News are presented in Table 5 . As predicted, base model performs the worst. It neither utilizes strategies to identify semantic clusters nor considers fluency when selecting paths, resulting in relatively low performance. Both our TTR-based and distance-based methods show improvements in results. Specifically, TTR-based method performs better than distance-based method. This is because TTR-based method directly extracts fundamental features from raw texts, whereas distance-based method relies on sentence-correlation features, which are more complex and may conflict with raw text.", "section": "Ablation", "sec_num": "5.1"}, {"text": "The addition of fluency constraint also contributes to the improvement of results. Furthermore, we observed that if neither TTR-based nor distancebased method is used, incorporating the fluency factor leads to a significant improvement. However, the improvement is less pronounced when TTR or distance is already employed. This observation highlights the effectiveness of TTR-based and distance-based method from another perspective. A cluster of closely related sentences makes it easier to summarize fluently, thereby reducing the importance of the fluency factor. ", "section": "Ablation", "sec_num": "5.1"}, {"text": "As described in \u00a73.3, we generate summary sentences in a non-neural way. In this section, we explore the effectiveness of replacing our approach with neural approaches. Specifically, we adopt the approach proposed by Ghalandari et al. (2022) and directly use their trained models (newsroom-L11 and newsroom-P75) that were trained on Newsroom (Grusky et al., 2018) . Additionally, we trained two new models on Multi-News using their training strategies. We integrate these neural models into step 3 of GLIMMER while keeping the other steps unchanged. Please refer to Table 6 for ROUGE scores obtained on Multi-News and Appendix G for details about these neural models.", "section": "Cluster Summarization by Neural Models", "sec_num": "5.2"}, {"text": "We find that using neural models for generation does not lead to improvements in ROUGE scores. Neural models require large corpora and substantial computing resources for training, which is not aligned with our goals. Moreover, the effectiveness of text generation is highly dependent on the parameter settings during training, resulting in limited generalization capabilities.", "section": "Cluster Summarization by Neural Models", "sec_num": "5.2"}, {"text": "ChatGPT demonstrates remarkable dialogue abilities and can be applied to various tasks, including summarization. The fundamental principles of ChatGPT align with InstructGPT (Ouyang et al., 2022) , utilizing reinforcement learning from human feedback (RLHF) to optimize the language model. ChatGPT is not only pre-trained on a largescale corpus but also fine-tuned using supervised data, with significant human effort dedicated to training a reward model. Given that ChatGPT is regularly updated and publicly available, it is highly likely that ChatGPT has been exposed to some of datasets used in our experiments. Nevertheless, we consider ChatGPT to represent an upper bound for GLIMMER.", "section": "Comparison with ChatGPT", "sec_num": "5.3"}, {"text": "We use GPT-3.5-based ChatGPT for experiments, specifically, we employ gpt-3.5-turbo due to its capability and cost-effectiveness. Additionally, we utilize text-davinci-003 for further comparison. Due to API limitations, we tested only the first 50 samples from each dataset. Prompt and more detailed information can be found in Appendix E.1.", "section": "Comparison with ChatGPT", "sec_num": "5.3"}, {"text": "Comparison results with ChatGPT based on ROUGE scores are presented in Table 7 . Despite facing a powerful language model, our approach still performs well, particularly on Multi-News and However, regarding DUC-2004, which was released earlier, we hypothesize that ChatGPT may have already been trained on these texts, resulting in substantially better performance compared to other datasets.", "section": "Comparison with ChatGPT", "sec_num": "5.3"}, {"text": "Human evaluation is conducted to assess the readability of summaries generated by ChatGPT. Our findings indicate that both gpt-3.5-turbo and exhibit higher fluency, coherence, and referential clarity. However, a notable disadvantage is the potential for ChatGPT to produce unfaithful outputs, with a higher likelihood observed when using the text-davinci-003 model. It is evident that each model mentioned exhibits trade-offs among different indicators, and it remains challenging to identify a summary model that excels in all aspects. For detailed human evaluation results and examples of hallucinated outputs, please refer to E.2.", "section": "Comparison with ChatGPT", "sec_num": "5.3"}, {"text": "We visualize the clustering results of TTR-based and distance-based methods. We compare them with the base model which set the number of clusters to 9 because this setting achieves relatively good ROUGE scores on Multi-News.", "section": "Clustering Visualization", "sec_num": "5.4"}, {"text": "For TTR-based method, we reduce the dimenof eigenmatrix using PCA and UMAP. Since different cluster numbers correspond to different eigenmatrices, position distributions of nodes after dimensionality reduction varies. Please refer to Ap-F.1 for visualization results on Multi-News. Each node represents a sentence, and nodes of the same color indicate that they belong to the same semantic cluster. It is evident that TTR-based method produces better clustering results, while the base model appears to exhibit more random clustering.", "section": "Clustering Visualization", "sec_num": "5.4"}, {"text": "For distance-based method, we visualize the adjacency matrix calculated in \u00a73.1, as it determines the number of clusters based on the distances between nodes in the adjacency matrix. This visualization provides a more intuitive representation. Results of graph cut based on distance, shown in Appendix F.2, outperform the base model, indicatthe effectiveness of this method.", "section": "Clustering Visualization", "sec_num": "5.4"}, {"text": "Figure 2 shows comparisons between reference and generated summaries. Parts with the same color indicate similar meanings, while text that is underlined represents grammatical errors or redundancy.", "section": "Case Study", "sec_num": "5.5"}, {"text": "From these examples, we can conclude that compared to Summpip, the summary generated our approach is more comprehensive and contains less irrelevant information. Additionally, our approach exhibits fewer grammatical errors. Regarding PRIMERA, it tends to produce repeated sentences, resulting in high redundancy.", "section": "Case Study", "sec_num": "5.5"}, {"text": "Our proposed GLIMMER is effective and efficient for unsupervised multi-document summarization, requiring no additional data before use. By leveraging basic features of raw texts and mining semantic clusters, GLIMMER generates high-quality summaries. Results demonstrate that GLIMMER outperforms current unsupervised approaches and even state-of-the-art pre-trained models under zeroshot settings. Moreover, our approach shows competitiveness with ChatGPT on certain datasets. In future work, we plan to incorporate external knowledge to generate more abstractive summaries.", "section": "Conclusion", "sec_num": "6"}, {"text": "All data and codes used in this paper comply with the license for use. GLIMMER poses minimal storage and leakage risks because it has no trainable parameters, and inferring original texts from the generated summaries is almost impossible. However, there remains a small probability of generating biased or toxic texts during the path selection in word graphs.", "section": "Ethics Statement", "sec_num": "7"}, {"text": "Data collection approval was received from an ethics review board. The remuneration paid to the annotators exceeds the average salary level in the where they are located. ", "section": "Ethics Statement", "sec_num": "7"}, {"text": "Multi-XScience DUC-2004 model R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L gpt-3.", "section": "Multi-News", "sec_num": null}, {"text": "A.1 Datasets Details Multi-News (Fabbri et al., 2019) A widely used dataset for multi-document summarization, featuring high-quality news article summaries professionally written by editors. Multi-XScience (Lu et al., 2020) A more challenging dataset by focusing on scientific articles. Each source text comprises an article's abstract along with abstracts from its referenced articles. The summary is derived from the related work section of the main article. DUC-2004 A classic multi-document summarization dataset. Each documents consists of 10 news articles and 4 reference summaries.", "section": "A Experiments Details", "sec_num": null}, {"text": "First-n We extract the first n sentences from each article in a document set and combine them to form a summary of the set. This method works because summaries often contain key information found at the beginning of articles. LexRank (Erkan and Radev, 2004 ) A graph-based approach where sentences are represented as nodes, and edges represent similarities between sentences. selects summary sentences based on their relation to other sentences in the document. Centroid (Rossiello et al., 2017) A method based on identifying the most relevant words and calculating a centroid embedding. Sentences are scored based on their similarity to this centroid. Summpip (Zhao et al., 2020) An unsupervised pipeline designed for summarizing multiple documents. It performs well on various tasks including summarization. PEGASUS (Zhang et al., 2020a) A Transformerbased model designed for abstractive summariza-It is pre-trained on a large corpus using selfsupervised objectives. We use PEGASUS as a baseline under zero-shot settings. PRIMERA (Xiao et al., 2022) A pre-trained model specialized in multi-document representation, excelling in connecting and aggregating information across documents. We deploy PRIMERA under zero-shot settings, as well as few-shot settings using 10 and 100 examples for better comparative", "section": "A.2 Baselines Details", "sec_num": null}, {"text": "First We extract the first 2, 1, 1 sentence from each article in a document set for Multi-News, Multi-XScience, DUC-2004, respectively. LexRank Number of key sentences is set to 6, 3, 4 for Multi-News, Multi-XScience, DUC-2004, respectively. Centroid use glove-wiki-gigaword-1005 as word vectors. Similar to LexRank, we set the number of key sentences to 6, 3, 4 for three datasets.", "section": "A.3 Baselines Settings", "sec_num": null}, {"text": "We set the cluster number to 9, 7, 5 for three datasets. The minimal length of each summary sentence is no less than 6 words.", "section": "A.3 Baselines Settings", "sec_num": null}, {"text": "For few-shot PRIMERA with 10 and 100 examples, we train it for 200 and 100 epoches respectively. We set learning rate to 3e -5, and a strategy is adopted. Validation check will be conducted every 5 epochs.", "section": "A.3 Baselines Settings", "sec_num": null}, {"text": "Following Fabbri et al. (2019) , we truncate the input of Multi-News and DUC-2004 to 500 tokens, which is a commonly used pre-processing step. For sample with S source input documents, we extract the first N/S tokens from each source docwhere N is the total desired input length. Since some source documents may be shorter, we determine the number of tokens to extract from each document until the desired length reached. Regarding Multi-XScience, we do not truncate it because its average input length is not significantly different from 500 tokens. Additionwe truncate DUC-2004 to different lengths (500, 1000, 1500) and experiment with GLIMMER. The results are presented in Table 8 . As noted in (Fabbri et al., 2019) , increasing the input length does not significantly improve the results.", "section": "A.4 Length Control", "sec_num": null}, {"text": "For the summary length, to ensure fair comparison among baselines, we follow Xiao et al. (2022) and set a uniform output length. Specifically, we use lengths of 256, 128, and 128 for Multi-News, Multi-XScience, and DUC-2004, respectively. This is implemented by adjusting hyperparameters or truncating outputs accordingly.", "section": "A.4 Length Control", "sec_num": null}, {"text": "experiments regarding GLIMMER are conducted on a machine equipped with Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz, without utilizing any GPUs. The experiments regarding neural network models are conducted on NVIDIA GeForce RTX 2080 Ti. Regarding runtime, GLIMMER is several times or even tens of times faster than neural network models, including traditional neural networks, pretrained language models, and large language models. For example, we estimated the time required for LLMs on the Multi-News test set. The local vicuna-13b-v1.5 (Zheng et al., 2023) and chatglm3-6b (Du et al., 2022) took over 30 and hours respectively on an NVIDIA A100 40GB while online took even longer due to request limitations. In contrast, GLIMMER reduce the time to 2 hours (on an Intel Core i7-6700 CPU, due to the low CPU usage, parallel computing can be utilized).", "section": "A.5 Other Details", "sec_num": null}, {"text": "from few-shot fine-tuned results, fully supervised fine-tuned results are also crucial for understanding the extent to which our approach can be improved. Table 9 ", "section": "A.6 Fully Supervised Results", "sec_num": null}, {"text": "Figure 4 shows the distributions of human evaluation scores. Regarding fluency, the peak score for PRIMERA is 4, while scores 3 and 4 are both peaks for GLIMMER-TTR. Despite PRIMERA being a pre-trained language model, our unsupervised still has room for improvement in readability. However, in terms of informativeness, the peak score for GLIMMER-TTR is 5, whereas for it is 3, indicating that our approach is more consistent with the original text.", "section": "B.2 Score Distributions", "sec_num": null}, {"text": "compute Kendall's coefficient of concordance (Kendall-W) to measure inter-annotator agreement.", "section": "B.3 Agreement Analysis", "sec_num": null}, {"text": "assesses the agreement level among more than two annotators when scores are in rank order. According to Table 10 , annotators achieved moderate agreement on fluency, coherence, and referential clarity, and substantial agreement on informativeness.", "section": "B.3 Agreement Analysis", "sec_num": null}, {"text": "The main inconsistency among the three annotators is that non-native English speakers have lower tolerance for minor grammar errors compared to the native English speaker. Additionally, some annotators perceive continuously repeated sentences lacking fluency rather than incoherence. Nevertheless, we consider the agreement level among annotators to be acceptable, and the results of human evaluation are deemed reliable.", "section": "B.3 Agreement Analysis", "sec_num": null}, {"text": "UniEval was trained in the form of Boolean Question Answering, allowing it to evaluate summaries from multiple dimensions by providing different 11 . Among these metrics, fluency and coherence assess linguistic quality, indicating the readability of summaries, while consistency reflects the factual alignment between summaries and source documents.", "section": "C Evaluation by UniEval", "sec_num": null}, {"text": "Table 11 demonstrates that GLIMMER outperforms Summpip across all metrics. Despite GLIM-MER's linguistic quality being inferior to that of fully fine-tuned PRIMERA, it significantly excels consistency compared to PRIMERA. It's worth as a neural network evaluation framework, UniEval may provide inappropriate scores in some cases. For instance, UniEval struggles to properly evaluate the performance of zero-shot PRIMERA. Although we have confirmed that zero-PRIMERA sometimes generates continuously repeated sentences, UniEval finds it challenging to such instances and may still provide very high fluency and coherence scores, even exceeding manually written reference summaries. Therefore, we omit the results for zero-shot PRIMERA, and we only consider the use of UniEval as a suppleexperiment.", "section": "C Evaluation by UniEval", "sec_num": null}, {"text": "We use LexicalRichness Repeat step 1 for samples of 36 words, 37 words, up to 50 words. This allows us to plot a TTR curve respect to length. 3. Use Formula (3) to identify the most appropriate value of D that best fits the curve obtained in step 4. Repeat steps 1 to 3 three times and the average value of D.", "section": "D Estimation of D-Value", "sec_num": null}, {"text": "We utilize OpenAI API 8 to access ChatGPT. Apart from model and messages, hyperparameof API requests are consistent with official use case. 9 We set model to either gpt-3.5-turbo or text-davinci-003. For messages, we use \"please summarize the following content in less than n words:\" as prompt, where n is set to 256, 128, 128 for Multi-news, Multi-XScience, DUC-2004, respectively. During our experiments, the API's response times varied widely due to network conditions and the OpenAI server, making it challenging to draw definitive conclusions on response", "section": "E Detials of Comparison with ChatGPT E.1 Deployment Details", "sec_num": null}, {"text": "We conduct human evaluation based on three readindicators. Table 12 presents human evaluation results of GLIMMER-TTR and two Chat-GPT models on the first 50 samples of Multi-News. Despite their generally high readability, ChatGPT models may exhibit hallucinations. Some hallucinations are easily noticeable, for instance, textdavinci-003 might produce ungrammatical and inaccurate sentences from the outset. Other hallucinations are more subtle and can appear anywhere in summaries without obvious grammatical errors.", "section": "E.2 Human Evaluation of ChatGPT", "sec_num": null}, {"text": "of such cases are shown in Figure 5 , where both gpt-3.5-turbo and text-davinci-003 generate inaccurate content, highlighted in red. Ghalandari et al. (2022) using Newsroom (Grusky et al., 2018) to predict summary sentences of 11 tokens. Similar with newsroom-L11 but trained to reduce clusters to 75% of their original multi-news-P40 by us on Multi-News to reduce clusters to 40 multi-news-Gamma Unlike the previous three that use a Gaussian distribution to control compression length, lengths of semantic clusters identified by GLIMMER follows a Gamma distribution. Therefore, we modified the reward function for length control, setting \u03b1 to 2 and \u03b2 to 15.", "section": "E.2 Human Evaluation of ChatGPT", "sec_num": null}, {"text": "Our code will be released and licensed under Apache License 2.0. The framework dependencies include:", "section": "H Software and Licenses", "sec_num": null}, {"text": "\u2022 scikit-learn, 10 BSD 3-Clause ", "section": "H Software and Licenses", "sec_num": null}, {"text": "We use spaCy for implementation. https://spacy.io.", "section": "", "sec_num": null}, {"text": "We use NLTK for implementation. https://www.nltk. org.", "section": "", "sec_num": null}, {"text": "We use ROUGE-1.5.5 implemented by https://github. com/li-plus/rouge-metric.", "section": "", "sec_num": null}, {"text": "In implementation, deberta-xlarge-mnli is used to represent embeddings.", "section": "", "sec_num": null}, {"text": "https://github.com/RaRe-Technologies/ gensim-data", "section": "", "sec_num": null}, {"text": "Graduate students, two of whom are non-native English speakers and one is a native English speaker.", "section": "", "sec_num": null}, {"text": "https://github.com/LSYS/LexicalRichness", "section": "", "sec_num": null}, {"text": "https://github.com/li-plus/rouge-metric/blob/ master/LICENSE", "section": "", "sec_num": null}, {"text": "https://github.com/gaetangate/ text-summarizer/blob/master/LICENSE", "section": "", "sec_num": null}, {"text": "https://github.com/crabcamp/lexrank/blob/dev/", "section": "", "sec_num": null}, {"text": "https://github.com/allenai/PRIMER/blob/main/ LICENSE", "section": "", "sec_num": null}, {"text": "https://github.com/maszhongming/UniEval/blob/ main/LICENSE", "section": "", "sec_num": null}, {"text": "https://github.com/yaolu/Multi-XScience/blob/ master/LICENSE", "section": "", "sec_num": null}, {"text": "https://github.com/Alex-Fabbri/Multi-News/ blob/master/LICENSE.txt", "section": "", "sec_num": null}, {"text": "https://github.com/numpy/numpy/blob/main/ LICENSE.txt", "section": "", "sec_num": null}, {"text": "https://github.com/Weebly/OrderedSet/blob/ master/LICENSE", "section": "", "sec_num": null}, {"text": "https://github.com/matplotlib/matplotlib/ blob/main/LICENSE/LICENSE", "section": "", "sec_num": null}], "back_matter": [{"text": "This work is supported by Youth Innovation Promotion Association CAS (No.2021155).", "section": "Acknowledgments", "sec_num": null}], "ref_entries": {"FIGREF0": {"num": null, "text": "Figure 1: Illustration of GLIMMER. There are three basic steps: sentence graph construction, semantic cluster identification and cluster summarization.", "uris": null, "fig_num": "1", "type_str": "figure"}, "FIGREF1": {"num": null, "text": "Figure 3: Human evaluation guideline", "uris": null, "fig_num": "3", "type_str": "figure"}, "FIGREF2": {"num": null, "text": "", "uris": null, "fig_num": null, "type_str": "figure"}, "FIGREF3": {"num": null, "text": "https://platform.openai.com 9 https://platform.openai.com/docs/ G Neural Cluster Summarization Models neural models are employed to summarize semantic clusters: newsroom-L11 A model trained by", "uris": null, "fig_num": null, "type_str": "figure"}, "FIGREF5": {"num": null, "text": "Figure 6: Visualization of clustering by the base model and TTR-based method, using PCA and UMAP to reduce dimension.", "uris": null, "fig_num": "6", "type_str": "figure"}, "TABREF0": {"html": null, "type_str": "table", "num": null, "text": "Zero and few-shot results of PRIMERA on Multi-News.", "content": "<table><tr><td>L</td></tr></table>"}, "TABREF1": {"html": null, "type_str": "table", "num": null, "text": "BERTScore results of GLIMMER and baslines on Multi-News. Baselines are non-neural SOTA Summpip and neural SOTA PRIMERA. Both zero-shot and fully fine-tuned PRIMERA are included.", "content": "<table><tr><td>model</td><td>P</td><td>R</td><td>F1</td></tr><tr><td>Summpip</td><td colspan=\"3\">60.45 58.38 59.33</td></tr><tr><td>PRIMERA (0)</td><td colspan=\"3\">58.86 52.92 55.62</td></tr><tr><td>PRIMERA (full)</td><td colspan=\"3\">60.90 58.01 59.36</td></tr><tr><td>GLIMMER-TTR</td><td colspan=\"3\">59.79 60.43 59.96</td></tr><tr><td colspan=\"4\">GLIMMER-Distance 60.50 59.21 59.63</td></tr></table>"}, "TABREF2": {"html": null, "type_str": "table", "num": null, "text": "Results of Human Evaluation.", "content": "<table><tr><td>model</td><td colspan=\"4\">fluency coherence referential clarity informativeness</td></tr><tr><td>Summpip</td><td>2.95</td><td>1.91</td><td>2.08</td><td>2.50</td></tr><tr><td>PRIMERA</td><td>3.68</td><td>2.76</td><td>2.95</td><td>2.57</td></tr><tr><td>GLIMMER-TTR</td><td>3.54</td><td>2.50</td><td>2.83</td><td>3.67</td></tr><tr><td>GLIMMER-Distance</td><td>3.15</td><td>2.42</td><td>2.52</td><td>3.10</td></tr></table>"}, "TABREF3": {"html": null, "type_str": "table", "num": null, "text": "Ablation study on Multi-News, base means the basic structure of GLIMMER with clusters number a fixed value of 9 and ignoring fluency when selecting path.", "content": "<table><tr><td>model</td><td>R-1</td><td>R-2</td><td>R-L</td></tr><tr><td>base</td><td colspan=\"3\">38.23 10.93 18.38</td></tr><tr><td>w/ fluency</td><td colspan=\"3\">41.37 12.31 19.26</td></tr><tr><td>w/ distance</td><td colspan=\"3\">40.88 12.31 20.50</td></tr><tr><td colspan=\"4\">w/ distance, fluency 42.44 13.26 21.00</td></tr><tr><td>w/ TTR</td><td colspan=\"3\">42.05 12.96 20.61</td></tr><tr><td>w/ TTR, fluency</td><td colspan=\"3\">43.08 13.87 21.05</td></tr></table>"}, "TABREF4": {"html": null, "type_str": "table", "num": null, "text": "", "content": "<table><tr><td colspan=\"4\">: Results of adopting neural summarization</td></tr><tr><td>models.</td><td/><td/><td/></tr><tr><td>model</td><td>R-1</td><td>R-2</td><td>R-L</td></tr><tr><td>GLIMMER-TTR</td><td colspan=\"3\">43.08 13.87 21.05</td></tr><tr><td>newsroom-L11</td><td colspan=\"3\">33.11 8.31 16.51</td></tr><tr><td>newsroom-P75</td><td colspan=\"3\">41.01 11.31 18.98</td></tr><tr><td>multi-news-P40</td><td colspan=\"3\">35.76 9.58 16.91</td></tr><tr><td colspan=\"4\">multi-news-Gamma 33.70 8.87 16.17</td></tr></table>"}, "TABREF5": {"html": null, "type_str": "table", "num": null, "text": "Comparison results with ChatGPT.", "content": "<table/>"}, "TABREF6": {"html": null, "type_str": "table", "num": null, "text": "5-turbo  42.03 11.71 20.73 31.98 3.61 16.82 40.86 10.34  21.71 text-davinci-003 42.00 12.66 21.28 30.67 3.83 15.78 41.13 11.04 22.72 GLIMMER-TTR 43.97 14.86 21.47 31.97 4.24 16.05 35.27 7.66 20.31 GLIMMER-Distance 43.23 14.08 21.42 30.60 3.52 15.86 34.90 6.90 18.45", "content": "<table/>"}, "TABREF7": {"html": null, "type_str": "table", "num": null, "text": "Experiments with different truncated length ofDUC-2004.", "content": "<table><tr><td colspan=\"2\">Length</td><td>GLIMMER-TTR</td></tr><tr><td/><td>R-1</td><td>34.90</td></tr><tr><td>500</td><td>R-2</td><td>6.90</td></tr><tr><td/><td>R-L</td><td>18.45</td></tr><tr><td/><td>R-1</td><td>34.36</td></tr><tr><td>1000</td><td>R-2</td><td>6.56</td></tr><tr><td/><td>R-L</td><td>17.70</td></tr><tr><td/><td>R-1</td><td>34.87</td></tr><tr><td>1500</td><td>R-2</td><td>6.92</td></tr><tr><td/><td>R-L</td><td>18.01</td></tr></table>"}, "TABREF9": {"html": null, "type_str": "table", "num": null, "text": "Agreement analysis.", "content": "<table><tr><td>indicator</td><td colspan=\"2\">Kendall-W p-value</td></tr><tr><td>fluency</td><td>0.43</td><td>.003</td></tr><tr><td>coherence</td><td>0.51</td><td>.000</td></tr><tr><td>referential clarity</td><td>0.52</td><td>.000</td></tr><tr><td>informativeness</td><td>0.62</td><td>.000</td></tr><tr><td colspan=\"3\">questions. To a certain degree, it correlates with</td></tr><tr><td colspan=\"3\">human evaluation. Fluency, coherence, and con-</td></tr><tr><td colspan=\"3\">sistency scores evaluated by UniEval are shown in</td></tr><tr><td>Table</td><td/><td/></tr></table>"}, "TABREF11": {"html": null, "type_str": "table", "num": null, "text": "https://github.com/scikit-learn/scikit-learn/ blob/main/COPYING 11 https://github.com/pytorch/pytorch/blob/main/ LICENSE 12 https://github.com/nltk/nltk/blob/develop/ LICENSE.txt 13 https://github.com/LSYS/LexicalRichness/blob/ master/LICENSE 14 https://github.com/scipy/scipy/blob/main/ LICENSE.txt 15 https://github.com/networkx/networkx/blob/ main/LICENSE.txt 16 https://github.com/explosion/spaCy/blob/ master/LICENSE 17 https://github.com/RaRe-Technologies/gensim/ blob/develop/COPYING 18 https://github.com/RaRe-Technologies/ gensim-data/blob/master/LICENSE", "content": "<table><tr><td>\u2022 PyTorch, 11 Misc</td></tr><tr><td>\u2022 NLTK, 12 Apache 2.0</td></tr><tr><td>\u2022 LexicalRichness, 13 MIT</td></tr><tr><td>\u2022 SciPy, 14 BSD 3-Clause</td></tr><tr><td>\u2022 NetworkX, 15 BSD 3-Clause</td></tr><tr><td>\u2022 spaCy, 16 MIT</td></tr><tr><td>\u2022 Gensim, 17 LGPL 2.1</td></tr><tr><td>\u2022 Gensim-data, 18 LGPL 2.1</td></tr></table>"}, "TABREF12": {"html": null, "type_str": "table", "num": null, "text": "Human evaluation results of ChatGPT.", "content": "<table/>"}}}}
"""
evaluation.py

This module implements the Evaluation class for the GLIMMER unsupervised multi-document
summarization system. It computes automatic evaluation metrics, specifically ROUGE F1 scores
for ROUGE-1, ROUGE-2, and ROUGE-L, by comparing the generated summary against the reference
summary/ies. It uses the rouge-metric library (version 1.2.1) for ROUGE calculations and numpy
for aggregating and averaging scores.

Usage Example:
    from evaluation import Evaluation
    evaluator = Evaluation(model, reference_summaries, generated_summaries, config)
    metrics = evaluator.evaluate()
    print("Evaluation Metrics:", metrics)
"""

import logging
import numpy as np
from typing import Dict, Any, List, Union

# Import PyRouge from the rouge-metric library.
# This library is assumed to be available as per the required package "rouge-metric==1.2.1".
from rouge_metric import PyRouge


class Evaluation:
    """
    Evaluation class responsible for computing automatic evaluation metrics (ROUGE scores)
    for the summaries generated by GLIMMER in comparison with the reference summaries.

    Attributes:
        model: An instance of the GLIMMER summarization model (not directly used in scoring).
        references: A mapping (dictionary) of document identifiers to reference summary strings,
                    or a list of summary strings.
        generated: A mapping (dictionary) of document identifiers to generated summary strings,
                   or a list of summary strings.
        config: A configuration dictionary (from config.yaml) for logging and evaluation settings.
    """

    def __init__(
        self,
        model: Any,
        references: Union[Dict[str, str], List[str]],
        generated: Union[Dict[str, str], List[str]],
        config: Dict[str, Any] = None
    ) -> None:
        """
        Initialize the Evaluation instance.

        Args:
            model: An instance of the GLIMMER model. (This is stored for interface consistency.)
            references: Either a dictionary (mapping document IDs to reference summaries) or a list of reference summaries.
            generated: Either a dictionary (mapping document IDs to generated summaries) or a list of generated summaries.
            config: Optional configuration dictionary, typically parsed from config.yaml.
                    Defaults (if not provided) use logging level "INFO".
        """
        self.model = model
        self.config = config if config is not None else {}
        self.logger = logging.getLogger(__name__)
        log_level_str: str = self.config.get("logging", {}).get("level", "INFO")
        numeric_level: int = getattr(logging, log_level_str.upper(), logging.INFO)
        logging.basicConfig(level=numeric_level, format="%(asctime)s [%(levelname)s] %(message)s")

        # Normalize the input summaries to dictionaries for standardized pairing.
        if isinstance(references, list) and isinstance(generated, list):
            if len(references) != len(generated):
                self.logger.warning("Mismatched counts: %d references vs. %d generated summaries.",
                                    len(references), len(generated))
            # Use string indices as keys.
            self.references: Dict[str, str] = {str(i): ref for i, ref in enumerate(references)}
            self.generated: Dict[str, str] = {str(i): gen for i, gen in enumerate(generated)}
        elif isinstance(references, dict) and isinstance(generated, dict):
            self.references = references
            self.generated = generated
            # Log missing keys between generated and references.
            ref_keys = set(self.references.keys())
            gen_keys = set(self.generated.keys())
            missing_in_generated = ref_keys - gen_keys
            if missing_in_generated:
                self.logger.warning("Missing generated summaries for keys: %s", missing_in_generated)
            missing_in_references = gen_keys - ref_keys
            if missing_in_references:
                self.logger.warning("Missing reference summaries for keys: %s", missing_in_references)
        else:
            raise ValueError("Both references and generated summaries must be either lists or dictionaries.")

    def evaluate(self) -> Dict[str, float]:
        """
        Evaluate the generated summaries against the reference summaries using ROUGE metrics.
        For each summary pair, computes ROUGE-1, ROUGE-2, and ROUGE-L F1 scores and then averages
        these values over the entire dataset.

        Returns:
            A dictionary with keys:
                "rouge-1": Average ROUGE-1 F1 score.
                "rouge-2": Average ROUGE-2 F1 score.
                "rouge-l": Average ROUGE-L F1 score.
        """
        self.logger.info("Starting evaluation of generated summaries using ROUGE metrics.")

        try:
            # Initialize PyRouge with ROUGE-1, ROUGE-2 and ROUGE-L metrics.
            rouge_scorer = PyRouge(rouge_n=(1, 2), rouge_l=True)
        except Exception as e:
            self.logger.error("Could not initialize PyRouge: %s", str(e))
            raise

        rouge1_scores: List[float] = []
        rouge2_scores: List[float] = []
        rougel_scores: List[float] = []
        valid_pairs: int = 0

        # Iterate over each summary pair using the keys of the reference dictionary.
        for key in self.references:
            reference_summary: str = self.references.get(key, "").strip()
            generated_summary: str = self.generated.get(key, "").strip()

            if not reference_summary or not generated_summary:
                self.logger.warning("Empty summary detected for key '%s'; skipping this pair.", key)
                continue

            try:
                # Get ROUGE scores for the candidate and reference summary.
                # The returned dictionary is expected to have keys "ROUGE-1", "ROUGE-2", "ROUGE-L",
                # with sub-keys "f", "p", "r". We are interested in F1 ("f").
                scores: Dict[str, Dict[str, float]] = rouge_scorer.get_scores(generated_summary, reference_summary)
                rouge1_f: float = scores.get("ROUGE-1", {}).get("f", 0.0)
                rouge2_f: float = scores.get("ROUGE-2", {}).get("f", 0.0)
                rougel_f: float = scores.get("ROUGE-L", {}).get("f", 0.0)

                rouge1_scores.append(rouge1_f)
                rouge2_scores.append(rouge2_f)
                rougel_scores.append(rougel_f)
                valid_pairs += 1
            except Exception as ex:
                self.logger.error("Error computing ROUGE scores for key '%s': %s", key, str(ex))

        if valid_pairs == 0:
            self.logger.error("No valid summary pairs were evaluated. Returning zero scores.")
            return {"rouge-1": 0.0, "rouge-2": 0.0, "rouge-l": 0.0}

        avg_rouge1: float = float(np.mean(rouge1_scores))
        avg_rouge2: float = float(np.mean(rouge2_scores))
        avg_rougel: float = float(np.mean(rougel_scores))

        self.logger.info("Evaluation completed on %d summary pairs.", valid_pairs)
        self.logger.info("Average ROUGE-1 F1: %.4f", avg_rouge1)
        self.logger.info("Average ROUGE-2 F1: %.4f", avg_rouge2)
        self.logger.info("Average ROUGE-L F1: %.4f", avg_rougel)

        results: Dict[str, float] = {
            "rouge-1": avg_rouge1,
            "rouge-2": avg_rouge2,
            "rouge-l": avg_rougel
        }

        return results


# For standalone testing of the Evaluation module.
if __name__ == "__main__":
    # Sample configuration dictionary similar to config.yaml settings.
    sample_config: Dict[str, Any] = {
        "logging": {
            "level": "INFO"
        }
    }

    # Dummy model instance (not used in evaluation but required by the interface).
    class DummyModel:
        pass

    dummy_model = DummyModel()

    # Sample reference and generated summaries.
    sample_references: Dict[str, str] = {
        "doc1": "The quick brown fox jumps over the lazy dog.",
        "doc2": "An apple a day keeps the doctor away."
    }
    sample_generated: Dict[str, str] = {
        "doc1": "A quick brown fox leaps over a lazy dog.",
        "doc2": "Eating an apple every day keeps you healthy."
    }

    evaluator = Evaluation(dummy_model, sample_references, sample_generated, sample_config)
    evaluation_metrics = evaluator.evaluate()
    print("Evaluation Metrics:", evaluation_metrics)
